{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up Boto3 and Interacting with S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Import necessary libraries and read AWS credentials:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "# Import your credentials from your .cfg file\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('aws.cfg')\n",
    "\n",
    "aws_access_key = config['AWS']['aws_access_key_id']\n",
    "aws_secret_key = config['AWS']['aws_secret_access_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Create an S3 client:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the S3 client\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key\n",
    ")\n",
    "\n",
    "# Verify the client is set up correctly by listing buckets\n",
    "# response = s3.list_buckets()\n",
    "# print('Existing buckets:')\n",
    "# for bucket in response['Buckets']:\n",
    "#     print(f'  {bucket[\"Name\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. List all buckets and get the location of a specific bucket:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aryan-techcatalyst-awswrangler-lab has been created in us-west-2\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'aryan-techcatalyst-awswrangler-lab'\n",
    "# if not us-east-1\n",
    "location = config['AWS']['region_name']\n",
    "\n",
    "# YOUR CODE TO CREATE A BUCKET\n",
    "s3.create_bucket(Bucket=bucket_name)\n",
    "print(f'{bucket_name} has been created in {location}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Create a new bucket:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: /workspaces/boto3_lab/awswrangler/test.csv uploaded to bucket aryan-techcatalyst-awswrangler-lab with key upload_file/test_s3.csv.\n",
      "File uploaded to bucket aryan-techcatalyst-awswrangler-lab with key upload_file/test_s3.csv.\n"
     ]
    }
   ],
   "source": [
    "filename = '/workspaces/boto3_lab/awswrangler/test.csv'\n",
    "key = 'upload_file/test_s3.csv'\n",
    "\n",
    "# UPLOAD_FILE CODE\n",
    "s3.upload_file(filename, bucket_name, key)\n",
    "# Print a confirmation message\n",
    "print(f'File: {filename} uploaded to bucket {bucket_name} with key {key}.')\n",
    "\n",
    "\n",
    "#PUT_OBJECT CODE\n",
    "with open (filename, 'rb') as f:\n",
    "    s3.put_object(Bucket=bucket_name, Key=key, Body=f)\n",
    "# Print a confirmation message\n",
    "print(f'File uploaded to bucket {bucket_name} with key {key}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Upload files using `upload_file` and `put_object`: A file of your choice**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition to AWS SDK for Pandas (awswrangler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Install awswrangler:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: awswrangler in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.9.0)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.20.32 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from awswrangler) (1.34.145)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.23.32 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from awswrangler) (1.34.145)\n",
      "Collecting numpy<2.0,>=1.18 (from awswrangler)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: packaging<25.0,>=21.1 in /home/codespace/.local/lib/python3.10/site-packages (from awswrangler) (24.1)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from awswrangler) (2.2.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from awswrangler) (17.0.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.4.0 in /home/codespace/.local/lib/python3.10/site-packages (from awswrangler) (4.12.2)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from boto3<2.0.0,>=1.20.32->awswrangler) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from boto3<2.0.0,>=1.20.32->awswrangler) (0.10.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/codespace/.local/lib/python3.10/site-packages (from botocore<2.0.0,>=1.23.32->awswrangler) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from botocore<2.0.0,>=1.23.32->awswrangler) (2.0.7)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas<3.0.0,>=1.2.0->awswrangler) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.10/site-packages (from pandas<3.0.0,>=1.2.0->awswrangler) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.23.32->awswrangler) (1.16.0)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.1\n",
      "    Uninstalling numpy-2.0.1:\n",
      "      Successfully uninstalled numpy-2.0.1\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    }
   ],
   "source": [
    "# !pip install awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/python/3.10.13/lib/python3.10/site-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Using cached numpy-2.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awswrangler 3.9.0 requires numpy<2.0,>=1.18; python_version < \"3.12\", but you have numpy 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.0.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Import awswrangler and set up default session:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "\n",
    "boto3.setup_default_session(\n",
    "    aws_access_key_id=config['AWS']['aws_access_key_id'],\n",
    "    aws_secret_access_key=config['AWS']['aws_secret_access_key'],\n",
    "    region_name=config['AWS']['region_name']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Read data from S3 directly into a Pandas DataFrame:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = wr.s3.read_csv('s3://aryan-techcatalyst-awswrangler-lab/upload_file/test_s3.csv') \n",
    "except Exception as e:\n",
    "    print('error')\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Inspect the DataFrame: Check the type (is it a Pandas DataFrame), DataFrame Shape, print first few rows, inspect with info**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Write DataFrame to S3 as a Parquet file:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': ['s3://aryan-techcatalyst-awswrangler-lab/c32cd76f4f774255b6e44a7a6f51064f.snappy.parquet'],\n",
       " 'partitions_values': {}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# complete the code\n",
    "wr.s3.to_parquet(\n",
    "    df=df, \n",
    "    path= \"s3://aryan-techcatalyst-awswrangler-lab/\",\n",
    "    dataset=True,\n",
    "    mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Work with AWS Glue Catalog:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* List databases: Note this will be showing Glue databases based on the region you specified when you created the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Database Description\n",
      "0  studio_widget_input-stream_1721327027000_db            \n"
     ]
    }
   ],
   "source": [
    "databases = wr.catalog.databases()\n",
    "print(databases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Create a new database if it doesn't exist:** Call it <yourname>-awswrangler_test\n",
    "* REPLACE with actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Database Description\n",
      "0                       aryan_awswrangler_test            \n",
      "1  studio_widget_input-stream_1721327027000_db            \n"
     ]
    }
   ],
   "source": [
    "if \"aryan_awswrangler_test\" not in databases.values:\n",
    "    wr.catalog.create_database(\"aryan_awswrangler_test\")\n",
    "    print(wr.catalog.databases())\n",
    "else:\n",
    "    print(\"Database aryan_awswrangler_test already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **List tables in a database:** The one you created as well as another Database of your choice that is in Glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Database, Table, Description, TableType, Columns, Partitions]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "tables = wr.catalog.tables(name_contains=\"aryan\")\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Write DataFrame to Data Lake with Glue Catalog:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = \"This is the csv file that we used yesterday, expect I wrote it to parquet\"\n",
    "param = {\"source\": \"https://github.com/tatwan/TechCatalyst_DE/tree/main/activities/wk6/awswrangler\", \"class\": \"sample data\"}\n",
    "comments = {\n",
    "    \"name\" : \"The name of the person\",\n",
    "    \"favorite_num\" : \"The person's favorite number\"\n",
    "}\n",
    "\n",
    "res = wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path= \"s3://aryan-techcatalyst-awswrangler-lab/\",\n",
    "    dataset=True,\n",
    "    database= \"aryan_awswrangler_test\",\n",
    "    table=\"test_parquet\",\n",
    "    mode=\"overwrite\",\n",
    "    glue_table_settings=wr.typing.GlueTableSettings(description=desc, parameters =param, columns_comments=comments)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>Table</th>\n",
       "      <th>Description</th>\n",
       "      <th>TableType</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aryan_awswrangler_test</td>\n",
       "      <td>test_parquet</td>\n",
       "      <td>This is the csv file that we used yesterday, e...</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>name, favorite_num</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Database         Table  \\\n",
       "0  aryan_awswrangler_test  test_parquet   \n",
       "\n",
       "                                         Description       TableType  \\\n",
       "0  This is the csv file that we used yesterday, e...  EXTERNAL_TABLE   \n",
       "\n",
       "              Columns Partitions  \n",
       "0  name, favorite_num             "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.tables(database=\"aryan_awswrangler_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Database</th>\n",
       "      <th>Table</th>\n",
       "      <th>Description</th>\n",
       "      <th>TableType</th>\n",
       "      <th>Columns</th>\n",
       "      <th>Partitions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aryan_awswrangler_test</td>\n",
       "      <td>test_parquet</td>\n",
       "      <td>This is the csv file that we used yesterday, e...</td>\n",
       "      <td>EXTERNAL_TABLE</td>\n",
       "      <td>name, favorite_num</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Database         Table  \\\n",
       "0  aryan_awswrangler_test  test_parquet   \n",
       "\n",
       "                                         Description       TableType  \\\n",
       "0  This is the csv file that we used yesterday, e...  EXTERNAL_TABLE   \n",
       "\n",
       "              Columns Partitions  \n",
       "0  name, favorite_num             "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wr.catalog.tables(name_contains=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Query data using Athena:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     name  favorite_num\n",
      "0  Vrinda            22\n",
      "1   Tracy            28\n",
      "2  Gareth            23\n",
      "3   Chris            16\n",
      "4    Emma            14\n",
      "5  Carlos             7\n",
      "6  Cooper            11\n",
      "7  Praful             4\n",
      "8   David            33\n",
      "9  Shilpa             2\n"
     ]
    }
   ],
   "source": [
    "df_athena = wr.athena.read_sql_query(\n",
    "    sql=\"SELECT * FROM test_parquet LIMIT 10\",\n",
    "    database=\"aryan_awswrangler_test\",\n",
    "    ctas_approach=True\n",
    ")\n",
    "print(df_athena)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
